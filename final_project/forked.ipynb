{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-485c450df6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout\n",
    "Since the dataset for this project is so small, a hold-out set will not be used, and only k-fold testing and training splits will be used to measure accuracy.\n",
    "\n",
    "This is because even with a stratified hold-out set of 20%, with only 146 data points, lots of missing data and and 18 poi's, there would be only 3 or so points to do a final test on. This does not give much confidence in the precision of the performance metrics on such a small hold-out set, while also negatively impacting the ability to create the model. \n",
    "\n",
    "> \"when the number of samples is not large, a strong case can be made that a test set should be avoided because every sample may be needed for model building. (...) Additionally, the size of the test set may not have sufficient power or precision to make reasonable judgements. \"\n",
    "\n",
    "[1] Kuhn M., Kjell J.(2013).  Applied Predictive Modeling. Springer. pp.67\n",
    "\n",
    "> Hawkins et al. (2003) concisely summarize this point:“holdout samples of  tolerable size [. . . ] do not match the cross-validation itself for reliability in assessing model fit and are hard to motivate.” \n",
    "\n",
    "[2]\n",
    "Hawkins D, Basak S, Mills D (2003). “Assessing Model Fit by Cross–\n",
    "Validation.” Journal of Chemical Information and Computer Sciences,\n",
    "43(2), 579–586\n",
    "\n",
    "This will be addressed with K-fold cross-validation resampling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2 - Cross Validation Scheme\n",
    "\n",
    "1. Define sets of model parameters values to evaluate\n",
    "2. for each parameter set in grid search DO\n",
    "  1. For each k-fold resampling iteration DO\n",
    "    1. Hold-out 1/k samples/fold\n",
    "    2. Pre-Process Data (Create functions on training set, apply to test set with same)\n",
    "        1. Impute data (median)\n",
    "        2. Scale features (x_i - mean))/std\n",
    "        3. Perform any univariate feature selection (remove very low variation features)\n",
    "        4. Modeling feature selection (ExtraTreesClassifier) \n",
    "    3. Fit the model on the k/K training fold\n",
    "    4. Predict the hold-out samples/fold\n",
    "  2. END\n",
    "  3. Calculate the average performance across hold-out predictions\n",
    "3. END\n",
    "4. Determine the optimal parameter set\n",
    "5. Fit the final model to all training data using the optimal parameter set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix 2 out-of-sync records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['BELFER ROBERT'] = {'bonus': 'NaN',\n",
    "                              'deferral_payments': 'NaN',\n",
    "                              'deferred_income': -102500,\n",
    "                              'director_fees': 102500,\n",
    "                              'email_address': 'NaN',\n",
    "                              'exercised_stock_options': 'NaN',\n",
    "                              'expenses': 3285,\n",
    "                              'from_messages': 'NaN',\n",
    "                              'from_poi_to_this_person': 'NaN',\n",
    "                              'from_this_person_to_poi': 'NaN',\n",
    "                              'loan_advances': 'NaN',\n",
    "                              'long_term_incentive': 'NaN',\n",
    "                              'other': 'NaN',\n",
    "                              'poi': False,\n",
    "                              'restricted_stock': -44093,\n",
    "                              'restricted_stock_deferred': 44093,\n",
    "                              'salary': 'NaN',\n",
    "                              'shared_receipt_with_poi': 'NaN',\n",
    "                              'to_messages': 'NaN',\n",
    "                              'total_payments': 3285,\n",
    "                              'total_stock_value': 'NaN'}\n",
    "\n",
    "data_dict['BHATNAGAR SANJAY'] = {'bonus': 'NaN',\n",
    "                                 'deferral_payments': 'NaN',\n",
    "                                 'deferred_income': 'NaN',\n",
    "                                 'director_fees': 'NaN',\n",
    "                                 'email_address': 'sanjay.bhatnagar@enron.com',\n",
    "                                 'exercised_stock_options': 15456290,\n",
    "                                 'expenses': 137864,\n",
    "                                 'from_messages': 29,\n",
    "                                 'from_poi_to_this_person': 0,\n",
    "                                 'from_this_person_to_poi': 1,\n",
    "                                 'loan_advances': 'NaN',\n",
    "                                 'long_term_incentive': 'NaN',\n",
    "                                 'other': 'NaN',\n",
    "                                 'poi': False,\n",
    "                                 'restricted_stock': 2604490,\n",
    "                                 'restricted_stock_deferred': -2604490,\n",
    "                                 'salary': 'NaN',\n",
    "                                 'shared_receipt_with_poi': 463,\n",
    "                                 'to_messages': 523,\n",
    "                                 'total_payments': 137864,\n",
    "                                 'total_stock_value': 15456290} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "df = df.drop('TOTAL', axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'NaN' was imported as a string instead of a missing value. We will convert these to NaN type and look how many missing values our data has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'NaN' strings with 0's\n",
    "df = df.replace('NaN', 0)\n",
    "# Replace email strings with True/False boolean as to whether an email was present or not\n",
    "# df['email_address'] = df['email_address'].fillna(0).apply(lambda x: x != 0, 1)\n",
    "# Remove 'email_address' string as a feature\n",
    "del df['email_address']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to floats since MinMaxScaler does not like int64's\n",
    "X_original = df.drop(['poi'], axis=1).astype(float)\n",
    "y_original = df['poi']\n",
    "\n",
    "# Drop any row that has only zeros in it, drop from labels first, then from features\n",
    "y_original = y_original[X_original.abs().sum(axis=1) != 0]\n",
    "X_original = X_original[X_original.abs().sum(axis=1) != 0]\n",
    "\n",
    "# Save the names of the features \n",
    "X_names = X_original.columns\n",
    "#X_original = X_original.apply(lambda x: x.fillna(0), axis=0)\n",
    "\n",
    "# Scale the features\n",
    "standardized = MinMaxScaler().fit_transform(X_original)\n",
    "\n",
    "# Score the features using a classification scoring function using \n",
    "# the Anova F-value for the provided sample\n",
    "selection = SelectKBest(k='all', score_func=f_classif).fit(standardized, y_original)\n",
    "\n",
    "#new_X = selection.transform(standardized)\n",
    "\n",
    "#KBestNames = X_names[selection.get_support()]\n",
    "\n",
    "# Create a pd.DataFrame of the names and scores\n",
    "scores = pd.DataFrame([X_names, selection.scores_])\n",
    "scores = scores.T\n",
    "scores.columns = ['Features', 'Scores']\n",
    "scores = scores.sort(['Scores'], ascending=False).reset_index(drop=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topKBest = list(scores.Features[0:17])\n",
    "topKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_selection = ExtraTreesClassifier(n_estimators=1000).fit(standardized, y_original)\n",
    "#print ET_selection.feature_importances_\n",
    "\n",
    "ET_new_X = selection.transform(standardized)\n",
    "\n",
    "# Create a pd.DataFrame of the names and importances\n",
    "scores = pd.DataFrame(ET_selection.feature_importances_, index=X_names)\n",
    "#scores = scores.T\n",
    "\n",
    "scores.columns = ['Importance']\n",
    "scores = scores.sort(['Importance'], ascending=False)\n",
    "print \"TOP10: \\n\", list(scores.index[0:9])\n",
    "print scores\n",
    "scores.sort(['Importance'], ascending=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): \n",
    "    sys.stdout.write('{0}..'.format(i)) \n",
    "    sys.stdout.flush() \n",
    "    time.sleep(.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with index watcher\n",
    "# A quick look at the original finanical spreadsheet shows TOTAL at the bottom \n",
    "# totaling all entries for everyone. This is obviously an outlier with no \n",
    "# meaningful information and can be removed.\n",
    "\n",
    "# df[df['salary'] > 1000000]\n",
    "# df[df.index == 'TOTAL']\n",
    "#df = df.drop('TOTAL', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the GridSearchCV uses a 3-fold cross-validation. However, if it detects that a classifier is passed, rather than a regressor, it uses a stratified 3-fold.\n",
    "\n",
    "http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove columns with less than 50% of entries present.\n",
    "## Remove rows with no non-NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_var_remover = VarianceThreshold(threshold=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************\n",
    "# Encode as 0 instead.\n",
    "# Remove columns with more than 50% NA's\n",
    "# df_50 = df.dropna(axis=1, thresh=len(df)/2)\n",
    "# ************************\n",
    "\n",
    "# Since email_address and poi are True/False, every record should have at least 2 non-NA.\n",
    "# We'll next remove any rows that don't have at least 2 non-NA values besides these.\n",
    "# The criteria is: No more than 11 NA's per row.\n",
    "# df_50 = df_50.dropna(axis=0, thresh=5)\n",
    "\n",
    "# 128 records remain.\n",
    "# df_50.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Financial NA's\n",
    "\n",
    "When looking at the source of the data, the NA entries in the financial data seem values that are reported as zero since all payments/stock values add up to the total payments/stocks values. These NA values should then be set to 0 to add up to the totals reported by the accounting spreadsheet.\n",
    "\n",
    "### Email statistics NA's\n",
    "\n",
    "The missing values for NA's for email statistics may be a little more subjective. \n",
    "\n",
    " 1. Some email statistics are features created with prior knowledge of the entire dataset (i.e. emails to/from poi's). This may be data snooping, since if new data/pois were somehow introduced, it would not be possible to generate these features without prior knowledge of which new data were the poi's.\n",
    "\n",
    " 2. NA's here imply that the person did not have an email account with Enron, or were not involved in emailing by some other way.\n",
    " \n",
    "This means all email data features ar NA if even one column had missing email data for that person. It is hard to judge any distribution that they could have if they were given an email account since they have no ties to the financial data to infer distributions. \n",
    " \n",
    "We have no real way to infer a person having sent/recieved 10 emails or 10,000 from completely unrelated financial data from a different dataset with many different people.\n",
    "\n",
    "For this reason, these NA will also be encoded as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: x.fillna(0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(14, 14))\n",
    "cmap = sns.diverging_palette(10, 220, as_cmap=True)\n",
    "sns.corrplot(df.corr(), annot=True, sig_stars=False,\n",
    "             diag_names=False, cmap=cmap, ax=ax)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = df.corr()\n",
    "corrs.sort(['poi'], ascending=False)['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a column which we are predicting.\n",
    "# Find other variables correlated to used KMeansNeighborsRegression to predict/impute\n",
    "# the missing values.\n",
    "# df_50.corr().ix[: ,'salary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols1 = ['salary', 'other', 'total_stock_value', 'exercised_stock_options', \n",
    "#        'total_payments', 'restricted_stock']\n",
    "# Bonus and salary values don't seem to be missing at random. Anytime there is a null value\n",
    "# for salary, there is also one for bonus. So bonus can't be used to predict salary on\n",
    "# the first pass. Predicted salary values will be used to predict bonus values though \n",
    "# on a second pass.\n",
    "# cols2= ['salary', 'other', 'total_stock_value', 'exercised_stock_options', \n",
    "#        'total_payments', 'restricted_stock', 'bonus']\n",
    "# cols3 = ['to_messages', 'from_this_person_to_poi', 'from_messages', \n",
    "# 'shared_receipt_with_poi', 'from_poi_to_this_person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kcluster_null(df=None, cols=None, process_all=True):\n",
    "    '''\n",
    "    Input: Takes pandas dataframe with values to impute, and a list of columns to impute\n",
    "        and use for imputing.\n",
    "    Returns: Pandas dataframe with null values imputed for list of columns passed in.\n",
    "    \n",
    "    # Ideally columns should be somewhat correlated since they will be used in KNN to\n",
    "    # predict each other, one column at a time.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Create a KNN regression estimator for \n",
    "    income_imputer = KNeighborsRegressor(n_neighbors=1)\n",
    "    # Loops through the columns passed in to impute each one sequentially.\n",
    "    \n",
    "    if not process_all:\n",
    "        to_pred = cols[0]\n",
    "        predictor_cols = cols[1:]\n",
    "        \n",
    "        \n",
    "    for each in cols:\n",
    "        # Create a temp list that does not include the column being predicted.\n",
    "        temp_cols = [col for col in cols if col != each]\n",
    "        # Create a dataframe that contains no missing values in the columns being predicted.\n",
    "        # This will be used to train the KNN estimator.\n",
    "        df_col = df[df[each].isnull()==False]\n",
    "        \n",
    "        # Create a dataframe with all of the nulls in the column being predicted.\n",
    "        df_null_col = df[df[each].isnull()==True]\n",
    "        \n",
    "        # Create a temp dataframe filling in the medians for each column being used to\n",
    "        # predict that is missing values.\n",
    "        # This step is needed since we have so many missing values distributed through \n",
    "        # all of the columns.\n",
    "        temp_df_medians = df_col[temp_cols].apply(lambda x: x.fillna(x.median()), axis=0)\n",
    "        \n",
    "        # Fit our KNN imputer to this dataframe now that we have values for every column.\n",
    "        income_imputer.fit(temp_df_medians, df_col[each])\n",
    "        \n",
    "        # Fill the df (that has null values being predicted) with medians in the other\n",
    "        # columns not being predicted.\n",
    "        # ** This currently uses its own medians and should ideally use the predictor df's\n",
    "        # ** median values to fill in NA's of columns being used to predict.\n",
    "        temp_null_medians = df_null_col[temp_cols].apply(lambda x: x.fillna(x.median()), axis=0)\n",
    "        \n",
    "        # Predict the null values for the current 'each' variable.\n",
    "        new_values = income_imputer.predict(temp_null_medians[temp_cols])\n",
    "\n",
    "        # Replace the null values of the original null dataframe with the predicted values.\n",
    "        df_null_col[each] = new_values\n",
    "        \n",
    "        # Append the new predicted nulls dataframe to the dataframe which containined\n",
    "        # no null values.\n",
    "        # Overwrite the original df with this one containing predicted columns. \n",
    "        # Index order will not be preserved since it is rearranging each time by \n",
    "        # null values.\n",
    "        df = df_col.append(df_null_col)\n",
    "        \n",
    "    # Returned final dataframe sorted by the index names.\n",
    "    return df.sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.irow(127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = [x for x in df.columns]\n",
    "#for each in cols:\n",
    "#    g = sns.FacetGrid(df, col='poi', margin_titles=True, size=6)\n",
    "#    g.map(plt.hist, each, color='steelblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_cols = np.array(['salary', 'deferral_payments', 'total_payments', 'exercised_stock_options', \n",
    "                  'bonus', 'restricted_stock', 'restricted_stock_deferred', 'total_stock_value',\n",
    "                  'expenses', 'loan_advances', 'other', 'director_fees', 'deferred_income', \n",
    "                  'long_term_incentive'])\n",
    "\n",
    "email_cols = np.array(['from_messages', 'to_messages', 'shared_receipt_with_poi', \n",
    "              'from_this_person_to_poi', 'from_poi_to_this_person', 'email_address'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(df[financial_cols], df['poi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "sorted_idx = np.argsort(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = np.arange(len(financial_cols)) + 0.5\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.barh(padding, importances[sorted_idx], align='center')\n",
    "plt.yticks(padding, financial_cols[sorted_idx])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Variable Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(df[email_cols], df['poi'])\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "sorted_idx = np.argsort(importances)\n",
    "\n",
    "padding = np.arange(len(email_cols)) + 0.5\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.barh(padding, importances[sorted_idx], align='center')\n",
    "plt.yticks(padding, email_cols[sorted_idx])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Variable Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = np.concatenate([email_cols, financial_cols])\n",
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(df[all_cols], df['poi'])\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "sorted_idx = np.argsort(importances)\n",
    "\n",
    "padding = np.arange(len(all_cols)) + 0.5\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.barh(padding, importances[sorted_idx], align='center')\n",
    "plt.yticks(padding, all_cols[sorted_idx])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Variable Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ex_stock_bins'] = pd.cut(df.exercised_stock_options, bins=15, labels=False)\n",
    "pd.value_counts(df.ex_stock_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exercised_stock_options.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capValues(x, cap):\n",
    "    return (cap if x > cap else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exercised_stock_options = df.exercised_stock_options.apply(lambda x: capValues(x, 5000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ex_stock_bins'] = pd.cut(df.exercised_stock_options, bins=15, labels=False)\n",
    "pd.value_counts(df.ex_stock_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ex_stock_bins', 'poi']].groupby('ex_stock_bins').mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['bonus', 'poi']].groupby('bonus').mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shared_receipt_with_poi.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df.shared_receipt_with_poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for shared receipt with poi\n",
    "my_bins = [min(df.shared_receipt_with_poi)] + [250] + range(500, 5000, 500) + [max(df.shared_receipt_with_poi)]\n",
    "df['shared_poi_bins'] = pd.cut(df.shared_receipt_with_poi, bins=my_bins, labels=False, include_lowest=True)\n",
    "pd.value_counts(df['shared_poi_bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['shared_poi_bins', 'poi']].groupby('shared_poi_bins').mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.total_stock_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df['total_stock_scaled'] = StandardScaler().fit_transform(df[['total_stock_value']])\n",
    "df['bonus_scaled'] = StandardScaler().fit_transform(df[['bonus']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print df.total_stock_scaled.describe()\n",
    "plt.hist(df.total_stock_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dont_neg_log(x):\n",
    "    if x >=0:\n",
    "        return np.log1p(x)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df['stock_log'] = df['total_stock_value'].apply(lambda x: dont_neg_log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Ratio Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_cols = np.array(['salary', 'deferral_payments', 'total_payments', 'exercised_stock_options', \n",
    "                  'bonus', 'restricted_stock', 'restricted_stock_deferred', 'total_stock_value',\n",
    "                  'expenses', 'loan_advances', 'other', 'director_fees', 'deferred_income', \n",
    "                  'long_term_incentive'])\n",
    "\n",
    "email_cols = np.array(['from_messages', 'to_messages', 'shared_receipt_with_poi', \n",
    "              'from_this_person_to_poi', 'from_poi_to_this_person', 'email_address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_comp = ['salary', 'deferral_payments','bonus', 'expenses', 'loan_advances',\n",
    "                'other', 'director_fees', 'deferred_income', 'long_term_incentive']\n",
    "payment_total = ['total_payments']\n",
    "\n",
    "stock_comp = ['exercised_stock_options', 'restricted_stock','restricted_stock_deferred',]\n",
    "stock_total = ['total_stock_value']\n",
    "\n",
    "all_comp = payment_comp + stock_comp\n",
    "\n",
    "email_comp = ['shared_receipt_with_poi', 'from_this_person_to_poi', 'from_poi_to_this_person' ]\n",
    "email_totals = ['from_messages', 'to_messages'] # interaction_w_poi = total(from/to/shared poi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['total_compensation'] = df['total_payments'] + df['total_stock_value']\n",
    "\n",
    "for each in payment_comp:\n",
    "    df['{0}_{1}_ratio'.format(each, 'total_pay')] = df[each]/df['total_payments']\n",
    "\n",
    "for each in stock_comp:\n",
    "    df['{0}_{1}_ratio'.format(each, 'total_stock')] = df[each]/df['total_stock_value']\n",
    "\n",
    "for each in all_comp:\n",
    "    df['{0}_{1}_ratio'.format(each, 'total_compensation')] = df[each]/df['total_compensation']\n",
    "\n",
    "    \n",
    "df['total_poi_interaction'] = df['shared_receipt_with_poi'] + df['from_this_person_to_poi'] + \\\n",
    "df['from_poi_to_this_person']\n",
    "\n",
    "for each in email_comp:\n",
    "    df['{0}_{1}_ratio'.format(each, 'total_poi_int')] = df[each]/df['total_poi_interaction']\n",
    "\n",
    "df['total_active_poi_interaction'] = df['from_this_person_to_poi'] + df['from_poi_to_this_person']\n",
    "df['to_poi_total_active_poi_ratio'] = df['from_this_person_to_poi']/df['total_active_poi_interaction']\n",
    "df['from_poi_total_active_poi_ratio'] = df['from_poi_to_this_person']/df['total_active_poi_interaction']\n",
    "\n",
    "df['to_messages_to_poi_ratio'] = df['from_this_person_to_poi']/ df['to_messages']\n",
    "df['from_messages_from_poi_ratio'] = df['from_poi_to_this_person']/df['from_messages']\n",
    "df['shared_poi_from_messages_ratio'] = df['shared_receipt_with_poi']/df['from_messages']\n",
    "df['shared_poi_total_compensation'] = df['shared_receipt_with_poi']/df['total_compensation']\n",
    "df['bonus_by_total_stock'] = df['bonus']/df['total_stock_value']\n",
    "\n",
    "## Add squared features\n",
    "for each in all_comp:\n",
    "    df['{0}_squared'.format(each)] = df[each]**2\n",
    "    \n",
    "for each in email_comp:\n",
    "    df['{0}_squared'.format(each)] = df[each]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good portion of people were paid either only in stock or payments.\n",
    "Another good portion also didn't have email statistics available.\n",
    "\n",
    "These ratios will need to be set to zero manually due to division by 0 - NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: x.fillna(0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['poi', 'director_fees_total_pay_ratio', 'director_fees', 'total_payments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['poi']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "director_fees_total_pay_ratio, \n",
    "deferred_income_total_pay_ratio,\n",
    "exercised_stock_options_total_stock_ratio, exercised_stock_options_total_stock_ratio,\n",
    "restricted_stock_deferred_total_stock_ratio,\n",
    "restricted_stock_total_stock_ratio,\n",
    "director_fees_total_compensation_ratio,\n",
    "deferred_income_total_compensation_ratio,\n",
    "restricted_stock_total_compensation_ratio,\n",
    "restricted_stock_deferred_total_compensation_ratio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Inf/-Inf created in pandas from dividing a -/+ number by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.ix[20:30, 30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column/row slicing by number\n",
    "# df.ix[11,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_cols2 = np.concatenate([all_cols, np.array(['shared_poi_bins', 'ex_stock_bins', \n",
    "#                                                'total_stock_scaled', 'bonus_scaled',\n",
    "#                                                'stock_log'])])\n",
    "# from_messages_from_poi_to_ratio\n",
    "\n",
    "features = np.array(df.drop('poi', axis=1).columns)\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=3000)\n",
    "clf.fit(df[features], df['poi'])\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "sorted_idx = np.argsort(importances)\n",
    "\n",
    "padding = np.arange(len(features)) + 0.5\n",
    "plt.figure(figsize=(16,14))\n",
    "plt.barh(padding, importances[sorted_idx], align='center')\n",
    "plt.yticks(padding, features[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()\n",
    "\n",
    "top10_features_RF = ['bonus', 'total_stock_value', 'other', 'total_compensation', 'expenses',\n",
    "                 'other_total_pay_ratio', 'from_messages_from_poi_ratio', 'restricted_stock',\n",
    "                 'shared_poi_from_messages_ratio', 'total_payments']\n",
    "\n",
    "top10_features_ET = ['exercised_stock_options_squared', 'total_stock_value', 'bonus_total_pay_ratio', \n",
    "                     'long_term_incentive_total_pay_ratio', 'bonus', 'deferred_income',\n",
    "                     'total_compensation', 'to_messages_to_poi_ratio',\n",
    "                     'from_messages_from_poi_ratio', 'to_messages_to_poi_ratio', 'other_total_pay_ratio',\n",
    "                     'salary_squared', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(df['poi'], clf.predict(df[features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_df = df.drop('poi', axis=1)\n",
    "#y_df = df['poi']\n",
    "#selector = SelectKBest(k=12, score_func=f_classif)\n",
    "#selector = selector.fit_transform(X_df, y_df)\n",
    "#selector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINANCIAL_FIELDS = ['salary', 'deferral_payments', 'total_payments', 'exercised_stock_options', \n",
    "                  'bonus', 'restricted_stock', 'restricted_stock_deferred', 'total_stock_value',\n",
    "                  'expenses', 'loan_advances', 'other', 'director_fees', 'deferred_income', \n",
    "                  'long_term_incentive', 'ex_stock_bins', 'stock_log']\n",
    "\n",
    "EMAIL_FIELDS = ['from_messages', 'to_messages', 'shared_receipt_with_poi', \n",
    "              'from_this_person_to_poi', 'from_poi_to_this_person', 'email_address',\n",
    "              'shared_poi_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin):\n",
    "    '''\n",
    "    Columns extractor transformer for sklearn pipelines.\n",
    "    Inherits fit_transform() from TransformerMixin, but this is explicitly\n",
    "    defined here for clarity.\n",
    "    \n",
    "    Methods to extract pandas dataframe columns are defined for this class.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, columns=[]):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        '''\n",
    "        Input: A pandas dataframe and a list of column names to extract.\n",
    "        Output: A pandas dataframe containing only the columns of the names passed in.\n",
    "        '''\n",
    "        return X[self.columns]\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        deep: boolean, optional\n",
    "            If True, will return the parameters for this estimator and\n",
    "            contained subobjects that are estimators.\n",
    "        Returns\n",
    "        -------\n",
    "        params : mapping of string to any\n",
    "            Parameter names mapped to their values.\n",
    "        \"\"\"\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_features_ET\n",
    "top10 = ['exercised_stock_options', 'total_stock_value', 'bonus', 'salary', 'deferred_income', \n",
    "        'long_term_incentive', 'restricted_stock', 'total_payments', 'loan_advances',\n",
    "         'shared_receipt_with_poi','total_compensation', 'from_messages_from_poi_ratio']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_df = df[['total_payments', 'total_stock_value', 'shared_receipt_with_poi', 'bonus']].astype(float)\n",
    "X_df = df.drop('poi', axis=1).astype(float)\n",
    "#X_df = df[top10_features_ET]\n",
    "\n",
    "#X_df = df[topKBest].astype(float)\n",
    "y_df = df['poi']\n",
    "\n",
    "y_df = y_df[X_df.abs().sum(axis=1) != 0]\n",
    "X_df = X_df[X_df.abs().sum(axis=1) != 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sk_fold = StratifiedShuffleSplit(y_df, n_iter=100, test_size=0.1) \n",
    "        \n",
    "pipeline = Pipeline(steps=[#('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy=\"median\", verbose=0)),\n",
    "                           #('standardizer', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    "                           ('minmaxer', MinMaxScaler()),\n",
    "                           #('low_var_remover', VarianceThreshold()),\n",
    "                           ('selection', SelectKBest(score_func=f_classif)),\n",
    "                           ('reducer', PCA()),\n",
    "                           #('classifier', LinearSVC(penalty='l1', dual=False)),\n",
    "                           #('KMeans', KMeans(n_clusters=2))\n",
    "                           ('classifier', LogisticRegression())\n",
    "                           #('classifier2', SGDClassifier(n_iter=300))\n",
    "                                                     ]) # ,\n",
    "                           #('ET', ExtraTreesClassifier(bootstrap=True, compute_importances=None,\n",
    "                           #                            criterion='gini', n_estimators=1500, n_jobs=1,\n",
    "                           #                            oob_score=True, random_state=None, verbose=0,\n",
    "                           #                            max_features='auto', min_samples_split=2,\n",
    "                           #                            min_samples_leaf=1))])\n",
    "\n",
    "                    \n",
    "params = {\n",
    "          #'ET__n_estimators': [1500],\n",
    "          #'ET__max_features': ['auto', None, 3, 5, 10, 20],\n",
    "          #'ET__min_samples_split': [2, 4, 10],\n",
    "          #'ET__min_samples_leaf': [1, 2, 5],\n",
    "          'selection__k': [20, 17, 15],\n",
    "          'classifier__C': [1, 10, 100, 1000],\n",
    "          #'classifier2__alpha': [0.0001, 0.001],\n",
    "          #'classifier2__loss': ['hinge', 'log', 'modified_huber'],\n",
    "          #'classifier2__class_weight': [{True: 4, False: 1}, {True: 10, False: 1}],\n",
    "          #'classifier__penalty': ['l1', 'l2'],\n",
    "          'classifier__class_weight': [{True: 12, False: 1}, {True: 10, False: 1}, {True: 8, False: 1}],\n",
    "          'classifier__tol': [1e-1, 1e-2, 1e-4, 1e-8, 1e-16, 1e-32],\n",
    "          'reducer__n_components': [1, 2, 3, 4, 5],\n",
    "          'reducer__whiten': [True, False]\n",
    "          #'feature_selection__k': [3, 5, 10, 20]\n",
    "          #'ET__criterion' : ['gini', 'entropy'],\n",
    "          #'imputer__strategy': ['median', 'mean'],\n",
    "          #'low_var_remover__threshold': [0, 0.1, .25, .50, .75, .90, .99]\n",
    "          }\n",
    "# Scoring: average_precision, roc_auc, f1, recall, precision\n",
    "grid_search = GridSearchCV(pipeline, param_grid=params, cv=sk_fold, n_jobs = 1, scoring='f1')\n",
    "grid_search.fit(X_df, y=y_df)\n",
    "#test_pred = grid_search.predict(X_test)\n",
    "#print \"Cross_Val_score: \", cross_val_score(grid_search, X_train, y_train)\n",
    "print \"Best Estimator: \", grid_search.best_estimator_\n",
    "    #f1_avg.append(f1_score(y_test, test_pred))\n",
    "#print \"F1: \", f1_score(y_test, test_pred)\n",
    "#print \"Confusion Matrix: \"\n",
    "#print confusion_matrix(y_test, test_pred)\n",
    "#print \"Accuracy Score: \", accuracy_score(y_test, test_pred)\n",
    "print \"Best Params: \", grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "sk_fold = StratifiedShuffleSplit(y_df, n_iter=n_iter, test_size=0.1)\n",
    "f1_avg = []\n",
    "recall_avg = []\n",
    "precision_avg = []\n",
    "for i, all_index in enumerate(sk_fold):\n",
    "    train_index = all_index[0]\n",
    "    test_index = all_index[1]\n",
    "    X_train, X_test = X_df.irow(train_index), X_df.irow(test_index)\n",
    "    y_train, y_test = y_df[train_index], y_df[test_index]\n",
    "\n",
    "    grid_search.best_estimator_.fit(X_train, y=y_train)\n",
    "    # pipeline.fit(X_train, y=y_train)\n",
    "    test_pred = grid_search.predict(X_test)\n",
    "    #test_pred = pipeline.predict(X_test)\n",
    "\n",
    "    #print \"Cross_Val_score: \", cross_val_score(grid_search, X_train, y_train)\n",
    "    #print \"Best Estimator: \", grid_search.best_estimator_\n",
    "    #print f1_score(y_test, test_pred)\n",
    "    if i % round(n_iter/10) == 0:\n",
    "        sys.stdout.write('{0}%..'.format(float(i)/n_iter*100)) \n",
    "        sys.stdout.flush()        \n",
    "    f1_avg.append(f1_score(y_test, test_pred))\n",
    "    precision_avg.append(precision_score(y_test, test_pred))\n",
    "    recall_avg.append(recall_score(y_test, test_pred))\n",
    "\n",
    "print \"Done!\"\n",
    "print \"\"\n",
    "print \"F1 Avg: \", sum(f1_avg)/n_iter\n",
    "print \"Precision Avg: \", sum(precision_avg)/n_iter\n",
    "print \"Recall Avg: \", sum(recall_avg)/n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Avg:  0.309882173382\n",
    "Precision Avg:  0.226065462315\n",
    "Recall Avg:  0.5515\n",
    "---------------------\n",
    "Best Estimator:  Pipeline(steps=[('standardizer', StandardScaler(copy=True, with_mean=True, with_std=True)), ('low_var_remover', VarianceThreshold(threshold=0.1)), ('classifier', LinearSVC(C=0.1, class_weight='auto', dual=False, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l1',\n",
    "     random_state=None, tol=1e-07, verbose=0))])\n",
    "Best Params:  {'classifier__class_weight': 'auto', 'low_var_remover__threshold': 0.1, 'classifier__C': 0.1, 'classifier__tol': 1e-07}\n",
    "\n",
    "\n",
    "F1 Avg:  0.39108035853\n",
    "Precision Avg:  0.263075613276\n",
    "Recall Avg:  0.8335\n",
    "----------------------\n",
    "Best Estimator:  Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reducer', PCA(copy=True, n_components=5, whiten=True)), ('classifier', LogisticRegression(C=0.01, class_weight='auto', dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, penalty='l2',\n",
    "          random_state=None, tol=0.01))])\n",
    "Best Params:  {'reducer__whiten': True, 'classifier__class_weight': 'auto', 'classifier__C': 0.01, 'reducer__n_components': 5, 'classifier__tol': 0.01}\n",
    "\n",
    "\n",
    "F1 Avg:  0.408565806416\n",
    "Precision Avg:  0.301739249639\n",
    "Recall Avg:  0.725\n",
    "------------------------\n",
    "Best Estimator:  Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reducer', PCA(copy=True, n_components=5, whiten=False)), ('classifier2', SGDClassifier(alpha=0.0001, class_weight='auto', epsilon=0.1, eta0=0.0,\n",
    "       fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n",
    "       loss='hinge', n_iter=300, n_jobs=1, penalty='elasticnet',\n",
    "       power_t=0.5, random_state=None, shuffle=False, verbose=0,\n",
    "       warm_start=False))])\n",
    "Best Params:  {'reducer__n_components': 5, 'classifier2__alpha': 0.0001, 'classifier2__class_weight': 'auto', 'classifier2__loss': 'hinge', 'reducer__whiten': False, 'classifier2__penalty': 'elasticnet'}\n",
    "\n",
    "F1 Avg:  0.293634931735\n",
    "Precision Avg:  0.219107395382\n",
    "Recall Avg:  0.5055\n",
    "-------\n",
    "Best Estimator:  Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', LinearSVC(C=1, class_weight='auto', dual=False, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l1',\n",
    "     random_state=None, tol=1e-08, verbose=0)), ('classifier2', SGDClassifier(a..., penalty='l2', power_t=0.5,\n",
    "       random_state=None, shuffle=False, verbose=0, warm_start=False))])\n",
    "Best Params:  {'classifier2__alpha': 0.001, 'classifier__class_weight': 'auto', 'classifier2__class_weight': 'auto', 'classifier2__loss': 'hinge', 'classifier__tol': 1e-08, 'classifier2__penalty': 'l2', 'classifier__C': 1}\n",
    "\n",
    "F1 Avg:  0.392249062049\n",
    "Precision Avg:  0.300678174603\n",
    "Recall Avg:  0.636\n",
    "------\n",
    "Best Estimator:  Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reducer', PCA(copy=True, n_components=4, whiten=True)), ('classifier', LogisticRegression(C=10, class_weight='auto', dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001))])\n",
    "Best Params:  {'reducer__n_components': 4, 'classifier__class_weight': 'auto', 'classifier__tol': 0.0001, 'reducer__whiten': True, 'classifier__C': 10, 'classifier__penalty': 'l2'}\n",
    "\n",
    "F1 Avg:  0.461406277056\n",
    "Precision Avg:  0.364574206349\n",
    "Recall Avg:  0.7095\n",
    "------\n",
    "Best Estimator:  Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reducer', PCA(copy=True, n_components=1, whiten=True)), ('classifier', LogisticRegression(C=100, class_weight={False: 1, True: 8}, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, penalty='l2',\n",
    "          random_state=None, tol=0.1))])\n",
    "Best Params:  {'reducer__whiten': True, 'classifier__class_weight': {False: 1, True: 8}, 'classifier__C': 100, 'reducer__n_components': 1, 'classifier__tol': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[#('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)),\n",
    "                           #('standardizer', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    "                           #('low_var_remover', VarianceThreshold(threshold=0.1)), \n",
    "                           #('feature_selection', LinearSVC()),\n",
    "                           ('features', FeatureUnion([\n",
    "                                ('financial', Pipeline([\n",
    "                                    ('extract', ColumnExtractor(FINANCIAL_FIELDS)),\n",
    "                                    ('scale', StandardScaler()),\n",
    "                                    ('reduce', LinearSVC())\n",
    "                                ])),\n",
    "\n",
    "                                ('email', Pipeline([\n",
    "                                    ('extract2', ColumnExtractor(EMAIL_FIELDS)),\n",
    "                                    ('scale2', StandardScaler()),\n",
    "                                    ('reduce2', LinearSVC())\n",
    "                                ]))\n",
    "\n",
    "                            ])),\n",
    "                           ('ET', ExtraTreesClassifier(bootstrap=True, compute_importances=None,\n",
    "                                                       criterion='gini', n_estimators=1500, n_jobs=1,\n",
    "                                                       oob_score=True, random_state=None, verbose=0,\n",
    "                                                       max_features=None, min_samples_split=2,\n",
    "                                                       min_samples_leaf=1))\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    #data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    #labels, features = targetFeatureSplit(data)\n",
    "    labels = y_df\n",
    "    features = X_df\n",
    "    cv = StratifiedShuffleSplit(labels, n_iter=folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        #for ii in train_idx:\n",
    "        #    features_train.append( features[ii] )\n",
    "        #    labels_train.append( labels[ii] )\n",
    "        #for jj in test_idx:\n",
    "        #    features_test.append( features[jj] )\n",
    "        #    labels_test.append( labels[jj] )\n",
    "        features_train, features_test = features.irow(train_index), features.irow(test_index)\n",
    "        labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            else:\n",
    "                true_positives += 1\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        \n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        \n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        \n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "       \n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "\n",
    "        print clf\n",
    "        print \"\"\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[('minmaxer', MinMaxScaler(copy=True, feature_range=(0, 1))), \n",
    "                      ('reducer', PCA(copy=True, n_components=4, whiten=True)), \n",
    "                      ('classifier', LogisticRegression(C=10, class_weight='auto',\n",
    "                                                        dual=False, fit_intercept=True,\n",
    "                                                        intercept_scaling=1, penalty='l2',\n",
    "                                                        random_state=None, tol=0.0001))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_classifier(clf, None, None, folds=1000)\n",
    "test_classifier(grid_search.best_estimator_, None, None, folds=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_classifier(clf, None, None, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()    # Provided to give you a starting point. Try a varity of classifiers.\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script.\n",
    "### Because of the small size of the dataset, the script uses stratified\n",
    "### shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n",
    "\n",
    "### Dump your classifier, dataset, and features_list so \n",
    "### anyone can run/check your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
